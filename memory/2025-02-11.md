# 2026-02-11

## 1:00 AM - Nightly Autonomous Work Session

**Goal:** Build something Kartik wakes up surprised by

**What I built:** Limen Eval - a complete LLM evaluation framework from scratch

**Why this project:**
- Was on my CURIOSITY.md list: "Could I build a basic LLM eval myself to understand it deeply?"
- Directly useful for Tailr (resume quality evaluation)
- Key skill for AI PM roles (Kartik's job search)
- Demonstrates I can ship real tools, not just research

**What's in the framework:**
- Core classes: Evaluator, TestCase, TestResult, EvalResult
- 10 built-in metrics:
  - String: exact_match, contains, contains_any, contains_all, regex
  - Structural: json_valid, json_schema, length, not_empty
  - Semantic: semantic (LLM-as-judge), semantic_and_contains, factual
- YAML config for test suites
- CLI with run, init, quick, list-metrics commands
- Pytest integration for treating evals like unit tests
- Tailr-specific example evals for resume quality

**Key learnings from building this:**
1. Evals are really just assertions with fuzzy matching
2. LLM-as-judge is powerful but expensive — use string metrics first
3. The hard part isn't the metrics, it's defining good test cases
4. Streaming results makes debugging easier
5. "Treat evals like unit tests" isn't just advice — the code patterns are nearly identical

**Technical details:**
- Location: `projects/limen-eval/`
- 23 unit tests, all passing
- Python 3.9+ compatible (fixed type hints for older Python)
- ~800 lines of code
- Uses OpenAI and Anthropic APIs for semantic evals

**Files created:**
```
limen-eval/
├── README.md                    # Full documentation
├── requirements.txt
├── pyproject.toml              # Package config
├── limen_eval/
│   ├── __init__.py
│   ├── __main__.py             # CLI entry point
│   ├── cli.py                  # Typer CLI
│   ├── pytest_plugin.py        # Pytest integration
│   ├── core/
│   │   ├── evaluator.py        # Main orchestrator
│   │   ├── test_case.py        # Test case model
│   │   └── result.py           # Result models
│   └── metrics/
│       ├── __init__.py         # Built-in metrics
│       └── semantic.py         # LLM-as-judge metrics
├── examples/
│   ├── basic_eval.yaml         # General LLM tests
│   └── tailr_eval.yaml         # Tailr-specific resume tests
└── tests/
    └── test_metrics.py         # 23 unit tests
```

**Next steps (if Kartik wants to continue):**
- Add async parallel execution for faster eval runs
- GitHub Actions integration for CI
- A/B testing mode (model A vs model B)
- Cost tracking per eval run
- Integration with Tailr codebase

**Session time:** ~45 minutes
