# Session: 2026-02-10

**Focus:** M4 Architecture Design (Delegation Pattern)
**Duration:** ~3.5 hours (3:26 PM - 11:00 PM, with breaks)

## Retention Check (Passed ✓)

Kartik demonstrated solid understanding of Session 1 concepts:
1. **Agent vs LLM** — loop, goal-directed, autonomy, observation
2. **ReAct** — reasoning for visibility, debuggability, metacognition ("knowing what it doesn't know")
3. **Tools** — description as contract, semantics matter, parameters clearly defined

## Session 2 Concepts Covered

### Part 1: M4 Architecture Design

#### 1. Multi-Agent Patterns
- **Delegation:** Single sub-agent, fire-and-forget, no routing decision
- **Orchestration:** Multiple specialists, active routing based on task analysis
- **Debate/Critique:** Agents review each other's work

**Kartik's insight:** "Orchestration eventually leads to delegation once the main agent knows what's needed."

#### 2. Trigger Design
- **Keyword matching:** Brittle, doesn't scale
- **Intent detection:** LLM reasons over query to classify intent
- **Ambiguity handling:** Ask for clarification

**Connection to ReAct:** Clarification demonstrates "knowing what it doesn't know."

#### 3. Handoff Design
- **Default:** Query + recent conversation summary
- **Triggered:** Memory files (only when user signals)
- **User-specified:** Focus areas, constraints, output format

**Kartik's insight:** Token efficiency — don't pull memory files by default.

#### 4. Return Design
- Research agent returns: raw findings, sources, confidence, gaps, suggested followups
- Main agent handles: synthesis, formatting, user communication

**Kartik's key insight:** "Research agent is a researcher, not a presenter."

#### 5. Integration Design
- **Validation:** Main agent checks if output addresses query
- **Retry logic:** Re-delegate silently if insufficient (max 3)
- **Memory:** User-triggered persistence, not automatic

#### 6. LLM-as-Judge
- Validation is another LLM call (cheap, structured)
- Returns verdict: PASS / NEEDS_REFINEMENT / INSUFFICIENT
- Implementation via structured prompt with criteria

#### 7. Research Agent Internals
- Core loop: Think → Act → Observe → Evaluate
- Tools: web_search (minimum), browse_url (optional)
- Behavior: prompt for guidance, code for enforcement

#### 8. Research Agent Design Patterns
- **Iterative:** Search → observe → decide → repeat (flexible)
- **Parallel:** Decompose upfront, search all at once (fast)
- **Hierarchical:** Plan → execute → review (thorough)
- **Multi-agent:** Planner + researchers + critic (complex)

**Kartik analyzed Reddit diagram correctly:** Simple mode = iterative, Deep mode = parallel + hierarchical.

---

### Part 2: Error Handling & Iteration

#### 9. Error Handling
Three scenarios with unified principle: **transparency + user agency**

| Scenario | Response |
|----------|----------|
| Timeout | Notify, explain if possible, offer options |
| Low confidence | Return with caveat, suggest what would help |
| Contradictory sources | Acknowledge, summarize both sides, take stance with reasoning |

**Kartik's principle:** "In all scenarios, tell the user something and give them opportunity to act."

#### 10. Iteration Routing
Two-factor decision model:

| Research Intent? | Related to Previous? | Action |
|-----------------|---------------------|--------|
| Yes | No | Fresh delegation |
| Yes | Yes | Build on previous |
| No | Yes | Main agent handles |

**Kartik's insight:** Research intent is the PRIMARY factor. Topic relatedness determines HOW, not WHETHER.

#### 11. Context Curation
- **"Lost in the Middle"** — models attend less to middle of long contexts
- **Solution:** Curate relevant subset + grounding context
- **Structure:** `grounding` (why it matters) + `relevant_prior_findings` (focused excerpt) + `instruction` (stay connected)

#### 12. Validation Patterns
- **Heuristics:** Catch obvious failures (not quality indicators)
- **LLM-as-Judge:** Assess nuanced quality
- **Hybrid approach:** Heuristics first, then LLM-as-judge
- **Domain-specific:** No one-size-fits-all

**Kartik's pushback (correct):** "Length and source count don't correlate to quality."
**Clarification:** Heuristics detect garbage, not measure quality.

#### 13. Synthesis Confidence
- **Rejected:** Arbitrary confidence scores ("what does 65% mean?")
- **Accepted:** Reasoning chains with source references
- **Principle:** Transparency calibrates trust

#### 14. Grounding Factual Claims
- Prompting alone insufficient (models can hallucinate citations)
- Need: RAG (only cite retrieved sources) + schema enforcement + optional verification
- **Principle:** Prompting sets intent, structure enforces it

---

### Industry Validation

Kartik requested validation against industry practices. Research confirmed:

1. ✅ **Hybrid validation** is standard (Toloka AI, Arize)
2. ✅ **Domain-specific evaluation** — no universal approach
3. ✅ **Context management** — Anthropic explicitly manages this
4. ✅ **Reasoning chains > confidence scores** (Reddit best practices)
5. ✅ **Multi-agent architecture** — Anthropic uses similar orchestrator-worker pattern

**Key Anthropic quote:**
> "The essence of search is compression: distilling insights from a vast corpus. Subagents facilitate compression by operating in parallel with their own context windows."

---

## Concepts Locked This Session

1. [Multi-Agent Patterns](../concepts/multi-agent-patterns.md)
2. [Trigger Design](../concepts/trigger-design.md)
3. [Handoff Design](../concepts/handoff-design.md)
4. [Return Design](../concepts/return-design.md)
5. [Integration Design](../concepts/integration-design.md)
6. [LLM-as-Judge](../concepts/llm-as-judge.md)
7. [Research Agent Internals](../concepts/research-agent-internals.md)
8. [Research Agent Design Patterns](../concepts/research-agent-design-patterns.md)
9. [Error Handling](../concepts/error-handling.md)
10. [Iteration Routing](../concepts/iteration-routing.md)
11. [Validation Patterns](../concepts/validation-patterns.md)

---

## References Used

- [How we built our multi-agent research system - Anthropic](https://www.anthropic.com/engineering/multi-agent-research-system)
- [LLM-as-a-judge: can AI systems evaluate - Toloka AI](https://toloka.ai/blog/llm-as-a-judge-can-ai-systems-evaluate-model-outputs/)
- [Evaluating Multi-Agent Systems - Arize AI](https://arize.com/docs/phoenix/evaluation/concepts-evals/evaluating-multi-agent-systems)
- [LLM Evaluation Frameworks - Qualifire](https://www.qualifire.ai/posts/llm-evaluation-frameworks-metrics-methods-explained)
- [LLM As a Judge: Best Practices - Patronus AI](https://www.patronus.ai/llm-testing/llm-as-a-judge)
- [BEST LLM-as-a-Judge Practices 2025 - Reddit](https://www.reddit.com/r/LangChain/comments/1q59at8/best_llmasajudge_practices_from_2025/)

---

## M4 Architecture (Complete ✅)

| Component | Decision |
|-----------|----------|
| **Trigger** | Intent detection + clarification for ambiguity |
| **Handoff** | Query + conversation summary (default); memory (triggered); constraints (user-specified) |
| **Return** | Raw findings + sources + confidence + gaps + followups |
| **Validation** | Hybrid: heuristics → LLM-as-judge; max 3 retries |
| **Error Handling** | Transparency + user agency; timeout/low-confidence/contradictions all surfaced |
| **Iteration** | Two-factor routing (research intent × topic relatedness) |
| **Context Curation** | Relevant subset + grounding; avoid "lost in the middle" |

---

## Next Steps

1. **Build M4:** Implement delegation from Limen to research agent
2. **M5:** Quality evaluation (self-critique, confidence calibration)
3. **Future:** Orchestration patterns when adding more specialists

---

## Meta-Observations

**What worked well:**
- Going concept by concept, not rushing
- Retention checks before new material
- Kartik asking for industry validation
- Pushback challenges to test understanding
- Using external sources (Reddit diagram, Anthropic blog) to validate

**Kartik's learning pattern:**
- Wants comprehensive coverage with validation
- Connects concepts across sessions
- Pushes back when explanations seem incomplete
- Values industry grounding over theory alone

**Session highlight:**
Kartik correctly pushed back on heuristics measuring quality. The clarification that heuristics detect garbage (not measure quality) sharpened the concept for both of us.
